{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwokchunghim/Sentiment-Classification/blob/main/SemEval_2017_(Task_4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0es7h1XigLt0"
      },
      "source": [
        "#### Import necessary packages\n",
        "You may import more packages here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrIlN6M4g2Hx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f274d7-b1d6-474d-9b09-5db7a26ecbf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Run this cell only when using colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH7VnSBsqJXe",
        "outputId": "a36da564-4834-4fda-f2b7-3a1bd6cbd68c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mpbPy85gLt1",
        "outputId": "d06e5712-79ac-48fa-d160-45d2a53b2e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "import re\n",
        "import os\n",
        "import copy\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem.porter import *\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import defaultdict\n",
        "from nltk import pos_tag\n",
        "from sklearn.svm import SVC\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import transformers\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abL_pYJTFhJU"
      },
      "outputs": [],
      "source": [
        "#Set current working directory to '/content/drive/MyDrive/CS918 Assignment 2'\n",
        "os.chdir('/content/drive/MyDrive/CS918 Assignment 2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glLLgcgtgLt2"
      },
      "outputs": [],
      "source": [
        "# Define test sets\n",
        "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYH3CrSqgLt2"
      },
      "outputs": [],
      "source": [
        "# Skeleton: Evaluation code for the test sets\n",
        "def read_test(testset):\n",
        "    '''\n",
        "    readin the testset and return a dictionary\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    '''\n",
        "    id_gts = {}\n",
        "    with open(testset, 'r', encoding='utf8') as fh:\n",
        "        for line in fh:\n",
        "            fields = line.split('\\t')\n",
        "            tweetid = fields[0]\n",
        "            gt = fields[1]\n",
        "\n",
        "            id_gts[tweetid] = gt\n",
        "\n",
        "    return id_gts\n",
        "\n",
        "\n",
        "def confusion(id_preds, testset, classifier):\n",
        "    '''\n",
        "    print the confusion matrix of {'positive', 'negative'} between preds and testset\n",
        "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    :classifier: str, the name of the classifier\n",
        "    '''\n",
        "    id_gts = read_test(testset)\n",
        "\n",
        "    gts = []\n",
        "    for m, c1 in id_gts.items():\n",
        "        if c1 not in gts:\n",
        "            gts.append(c1)\n",
        "\n",
        "    gts = ['positive', 'negative', 'neutral']\n",
        "\n",
        "    conf = {}\n",
        "    for c1 in gts:\n",
        "        conf[c1] = {}\n",
        "        for c2 in gts:\n",
        "            conf[c1][c2] = 0\n",
        "\n",
        "    for tweetid, gt in id_gts.items():\n",
        "        if tweetid in id_preds:\n",
        "            pred = id_preds[tweetid]\n",
        "        else:\n",
        "            pred = 'neutral'\n",
        "        conf[pred][gt] += 1\n",
        "\n",
        "    print(''.ljust(12) + '  '.join(gts))\n",
        "\n",
        "    for c1 in gts:\n",
        "        print(c1.ljust(12), end='')\n",
        "        for c2 in gts:\n",
        "            if sum(conf[c1].values()) > 0:\n",
        "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
        "            else:\n",
        "                print('0.000     ', end='')\n",
        "        print('')\n",
        "\n",
        "    print('')\n",
        "\n",
        "\n",
        "def evaluate(id_preds, testset, classifier):\n",
        "    '''\n",
        "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
        "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    :classifier: str, the name of the classifier\n",
        "    '''\n",
        "    id_gts = read_test(testset)\n",
        "\n",
        "    acc_by_class = {}\n",
        "    for gt in ['positive', 'negative', 'neutral']:\n",
        "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
        "\n",
        "    catf1s = {}\n",
        "\n",
        "    ok = 0\n",
        "    for tweetid, gt in id_gts.items():\n",
        "        if tweetid in id_preds:\n",
        "            pred = id_preds[tweetid]\n",
        "        else:\n",
        "            pred = 'neutral'\n",
        "\n",
        "        if gt == pred:\n",
        "            ok += 1\n",
        "            acc_by_class[gt]['tp'] += 1\n",
        "        else:\n",
        "            acc_by_class[gt]['fn'] += 1\n",
        "            acc_by_class[pred]['fp'] += 1\n",
        "\n",
        "    catcount = 0\n",
        "    itemcount = 0\n",
        "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "\n",
        "    microtp = 0\n",
        "    microfp = 0\n",
        "    microtn = 0\n",
        "    microfn = 0\n",
        "    for cat, acc in acc_by_class.items():\n",
        "        catcount += 1\n",
        "\n",
        "        microtp += acc['tp']\n",
        "        microfp += acc['fp']\n",
        "        microtn += acc['tn']\n",
        "        microfn += acc['fn']\n",
        "\n",
        "        p = 0\n",
        "        if (acc['tp'] + acc['fp']) > 0:\n",
        "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
        "\n",
        "        r = 0\n",
        "        if (acc['tp'] + acc['fn']) > 0:\n",
        "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
        "\n",
        "        f1 = 0\n",
        "        if (p + r) > 0:\n",
        "            f1 = 2 * p * r / (p + r)\n",
        "\n",
        "        catf1s[cat] = f1\n",
        "\n",
        "        n = acc['tp'] + acc['fn']\n",
        "\n",
        "        macro['p'] += p\n",
        "        macro['r'] += r\n",
        "        macro['f1'] += f1\n",
        "\n",
        "        if cat in ['positive', 'negative']:\n",
        "            semevalmacro['p'] += p\n",
        "            semevalmacro['r'] += r\n",
        "            semevalmacro['f1'] += f1\n",
        "\n",
        "        itemcount += n\n",
        "\n",
        "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
        "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
        "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
        "\n",
        "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
        "\n",
        "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX1mjdETgLt5"
      },
      "source": [
        "#### Load training set, dev set and testing set\n",
        "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYIxbT7GgLt6"
      },
      "outputs": [],
      "source": [
        "# Load training set, dev set and testing set\n",
        "data = {}\n",
        "tweetids = {}\n",
        "tweetgts = {}\n",
        "tweets = {}\n",
        "\n",
        "for dataset in ['twitter-training-data.txt', 'twitter-dev-data.txt'] + testsets:\n",
        "    dataset_path = join('semeval-tweets', dataset)\n",
        "\n",
        "    data[dataset] = []\n",
        "    tweets[dataset] = []\n",
        "    tweetids[dataset] = []\n",
        "    tweetgts[dataset] = []\n",
        "    # write code to read in the datasets here\n",
        "    \n",
        "    with open(dataset_path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            fields = line.split('\\t')\n",
        "            tweetids[dataset].append(fields[0])\n",
        "            tweetgts[dataset].append(fields[1])\n",
        "            tweets[dataset].append(fields[2].rstrip(\"\\n\"))  #remove line breaks\n",
        "            data[dataset].append(fields)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg1gtAeqjbxE"
      },
      "source": [
        "## 1. Word Preprocessing\n",
        "\n",
        "We remove URLs, hashtags, user mentions, non-alphabetical characters (except spaces) as we believe that they do not contain muuch sentiment information. Then we tokenize the sentences, lower-case the words and use a Porter Stemmer for word stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahtEYJWzyUjH"
      },
      "outputs": [],
      "source": [
        "#POS Map to WordNet if lemmatising is used\n",
        "pos_tag_map = defaultdict(lambda : wn.NOUN)\n",
        "pos_tag_map['J'] = wn.ADJ\n",
        "pos_tag_map['V'] = wn.VERB\n",
        "pos_tag_map['R'] = wn.ADV\n",
        "\n",
        "#Instantiate lemmatizer, stemmer, and tokenizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "tk = TweetTokenizer()\n",
        "\n",
        "#Preprossing before embedding \n",
        "def word_preprocess(X, stemming=True, joining=True):\n",
        "\n",
        "    #Instantiate tokenizer for our tweets\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    #Remove url\n",
        "    X_temp = [re.sub('https?:\\/\\/\\S+|www\\.(\\w+\\.)+\\S+', ' ', X[i]) for i in range(len(X))]\n",
        "    #Remove usermention\n",
        "    X_temp = [re.sub('@\\S*', ' ', X_temp[i]) for i in range(len(X_temp))]\n",
        "    #Remove hashtag\n",
        "    X_temp = [re.sub('#\\S*', ' ', X_temp[i]) for i in range(len(X_temp))]\n",
        "    #Remove special characters and punctuations\n",
        "    X_temp = [re.sub('[^A-Za-z0-9 ]+', ' ', X_temp[i]) for i in range(len(X_temp))]\n",
        "    #Tokenize sentences \n",
        "    X_temp = [tokenizer(X_temp[i]) for i in range(len(X_temp))]\n",
        "    #Lowercase (as GloVe contains most words in lowercase e.g. thursday)\n",
        "    X_temp = [[word.lower() for word in X_temp[i]] for i in range(len(X_temp))]\n",
        "    #Stemming with Porter Stemming\n",
        "    if stemming:\n",
        "        X_temp = [[stemmer.stem(word) for word in X_temp[i]] for i in range(len(X_temp))]\n",
        "    #Or lemmatizing\n",
        "    else:\n",
        "        for i in range(len(X_temp)):\n",
        "            for k, (word, tag) in enumerate(pos_tag(X_temp[i])):\n",
        "                if word not in stopwords.words('english'):\n",
        "                    new_word = lemmatizer.lemmatize(word, pos_tag_map[tag[0]])\n",
        "                    X_temp[i][k] = new_word\n",
        "                else:\n",
        "                    X_temp[i][k] = \"\"\n",
        "    #Join them back\n",
        "    if joining == True:\n",
        "        X_temp = [' '.join(X_temp[i]) for i in range(len(X_temp))]\n",
        "\n",
        "    return X_temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFVEH5UXsssG"
      },
      "source": [
        "## 2. Vectorize tokens and model training\n",
        "\n",
        "We first encode the labels with a LabelEncoder for classification task. Then we use a tf-idf vectorizer (which is a bag-of-words approach), to represent our tokens in a large sparse matrix. Then we fit the model with a Multinomial Naive Bayes model and make predictions on the developing set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il_XAqAmN0q-"
      },
      "outputs": [],
      "source": [
        "#Load training and validation data (and test data later)\n",
        "X_train = tweets['twitter-training-data.txt']\n",
        "X_val = tweets['twitter-dev-data.txt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8XNp4WpOImn"
      },
      "outputs": [],
      "source": [
        "#Preprocess data \n",
        "X_train_bow = word_preprocess(X_train)\n",
        "X_val_bow = word_preprocess(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Nss7EgPwVd7"
      },
      "outputs": [],
      "source": [
        "#Encode our labels (gts)\n",
        "Encoder = LabelEncoder()\n",
        "\n",
        "y_train = tweetgts['twitter-training-data.txt']\n",
        "y_val = tweetgts['twitter-dev-data.txt']\n",
        "\n",
        "\n",
        "y_train = Encoder.fit_transform(y_train)\n",
        "y_val = Encoder.fit_transform(y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD-8MhJySDzN"
      },
      "source": [
        "## Bag of Words Approach\n",
        "\n",
        "We first develop two models with a Bag-of-Word (BoW) appraoch. We create sparse matrices with the TD-IDF vectorizer, and fit the data to a Naive Bayes Model and a Logistic Regression Model respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aro3C9pqSbCK"
      },
      "outputs": [],
      "source": [
        "def bow_train(X_train, X_val, y_train, y_val, model):\n",
        "    #Instantiate tf-idf vectorizer\n",
        "    tfidf_vector = TfidfVectorizer()\n",
        "\n",
        "    #Fit the tf-idf into our training data\n",
        "    tfidf_vector.fit(X_train)\n",
        "\n",
        "    #Transform both training and dev data\n",
        "    X_train = tfidf_vector.transform(X_train)\n",
        "    X_val = tfidf_vector.transform(X_val)\n",
        "\n",
        "    #Fit our model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    #Make predictions on our developing data and transform it back to the original label\n",
        "    pred = model.predict(X_val)\n",
        "    pred = Encoder.inverse_transform(pred)\n",
        "\n",
        "    #Zip the two lists of ids and our predictions into a single dictionary for evaludation\n",
        "    id_preds = {tweetids['twitter-dev-data.txt'][i]: pred[i] for i in range(len(pred))}\n",
        "\n",
        "    return id_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raJIscoyd7RL"
      },
      "outputs": [],
      "source": [
        "#Instantiate Naive Bayes Classifier model\n",
        "nbc = MultinomialNB(fit_prior=False)\n",
        "\n",
        "#Logistic Regression\n",
        "lr = LogisticRegression(multi_class='multinomial', max_iter=3000, random_state=42)\n",
        "\n",
        "#K-nearest Neighbor\n",
        "knn = KNeighborsClassifier(n_neighbors=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0T1DmrFThra"
      },
      "outputs": [],
      "source": [
        "#Make predictions for the 3 models\n",
        "nbc_preds = bow_train(X_train_bow, X_val_bow, y_train, y_val, nbc)\n",
        "lr_preds = bow_train(X_train_bow, X_val_bow, y_train, y_val, lr)\n",
        "knn_preds = bow_train(X_train_bow, X_val_bow, y_train, y_val, knn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3mZiKDyu1fL",
        "outputId": "4daa7cee-074a-475e-e6b8-3511444dc065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "semeval-tweets/twitter-dev-data.txt (Naive Bayes): 0.581\n"
          ]
        }
      ],
      "source": [
        "#Evaluate the Naive Bayes model\n",
        "evaluate(nbc_preds, \"semeval-tweets/twitter-dev-data.txt\", \"Naive Bayes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-wSZ49rUjii",
        "outputId": "1c26e8b3-6a42-444e-945c-767e0971ec81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "semeval-tweets/twitter-dev-data.txt (KNN): 0.324\n"
          ]
        }
      ],
      "source": [
        "evaluate(knn_preds, \"semeval-tweets/twitter-dev-data.txt\", \"KNN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3Jz2abQA5DY",
        "outputId": "5e094457-7d5e-48f2-eedb-1f3b8eff21a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "semeval-tweets/twitter-dev-data.txt (Logistic Regression): 0.609\n"
          ]
        }
      ],
      "source": [
        "evaluate(lr_preds, \"semeval-tweets/twitter-dev-data.txt\", \"Logistic Regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucQKkiMduM5p",
        "outputId": "bba5ce5c-9aad-4fb7-f5db-3284e1bf07e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            positive  negative  neutral\n",
            "positive    0.615     0.073     0.312     \n",
            "negative    0.069     0.613     0.318     \n",
            "neutral     0.216     0.162     0.622     \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Also we evaluate our results with the confusion matrix\n",
        "\n",
        "confusion(nbc_preds, \"semeval-tweets/twitter-dev-data.txt\", \"Naive Bayes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwp0QKcyKmoH",
        "outputId": "fdfc8e7f-d354-4b5a-8747-cd3059443c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            positive  negative  neutral\n",
            "positive    0.724     0.031     0.245     \n",
            "negative    0.062     0.674     0.264     \n",
            "neutral     0.218     0.163     0.619     \n",
            "\n"
          ]
        }
      ],
      "source": [
        "confusion(lr_preds, \"semeval-tweets/twitter-dev-data.txt\", \"Random\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aMvVrIRM0z-",
        "outputId": "05ead394-85e7-4209-8b96-0bc6f77ca8c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            positive  negative  neutral\n",
            "positive    0.734     0.056     0.211     \n",
            "negative    0.101     0.595     0.304     \n",
            "neutral     0.292     0.194     0.514     \n",
            "\n"
          ]
        }
      ],
      "source": [
        "confusion(knn_preds, \"semeval-tweets/twitter-dev-data.txt\", \"Logistic Regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDyXSJyk7a4C"
      },
      "source": [
        "### Sentence embeddings\n",
        "\n",
        "We attempt to improve our model performance with BERT sentence embeddings. We perform minimum sentence preprocessing to prevent information loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpW2TupY7rJd",
        "outputId": "c4161567-dea7-4239-9e06-8db70d3bdbd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.27.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PL8bNcEg7Wbf"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgbBvA6VVfoW"
      },
      "outputs": [],
      "source": [
        "#Preprossing before sentence embedding \n",
        "def sentence_preprocess(X):\n",
        "    #Remove url\n",
        "    X_temp = [re.sub('https?:\\/\\/\\S+|www\\.(\\w+\\.)+\\S+', ' ', X[i]) for i in range(len(X))]\n",
        "    #Remove usermention\n",
        "    X_temp = [re.sub('@\\S*', ' ', X_temp[i]) for i in range(len(X_temp))]\n",
        "    #Remove hashtag\n",
        "    X_temp = [re.sub('#\\S*', ' ', X_temp[i]) for i in range(len(X_temp))]\n",
        "\n",
        "    return X_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42Vt8WKW9Fw0"
      },
      "outputs": [],
      "source": [
        "#Reload un-processed train and test data\n",
        "X_train = tweets['twitter-training-data.txt']\n",
        "X_val = tweets['twitter-dev-data.txt']\n",
        "\n",
        "#Encode our labels (gts)\n",
        "Encoder = LabelEncoder()\n",
        "\n",
        "y_train = tweetgts['twitter-training-data.txt']\n",
        "y_val = tweetgts['twitter-dev-data.txt']\n",
        "\n",
        "y_train = Encoder.fit_transform(y_train)\n",
        "y_val = Encoder.fit_transform(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCz4FAwKWFpi"
      },
      "outputs": [],
      "source": [
        "X_train_p = sentence_preprocess(X_train)\n",
        "X_val_p = sentence_preprocess(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kbDr2U5XCMs"
      },
      "outputs": [],
      "source": [
        "def sentence_embed_train(X_train, X_val, y_train, y_val, model):\n",
        "\n",
        "    #Load pretrained SBERT \n",
        "    pretrained = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    #Encode with pre-trained models\n",
        "    X_train_embeddings = pretrained.encode(X_train)\n",
        "    X_val_embeddings = pretrained.encode(X_val)\n",
        "\n",
        "    #Fit our model\n",
        "    model.fit(X_train_embeddings, y_train)\n",
        "\n",
        "    # Make predictions on our developing data and transform it back to the original label\n",
        "    pred = model.predict(X_val_embeddings)\n",
        "    pred = Encoder.inverse_transform(pred)\n",
        "\n",
        "    # Zip the two lists of ids and our predictions into a single dictionary for evaludation\n",
        "    id_preds = {tweetids['twitter-dev-data.txt'][i]: pred[i] for i in range(len(pred))}\n",
        "\n",
        "    return id_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD_4bnghXVFg"
      },
      "outputs": [],
      "source": [
        "#Instantiate new models\n",
        "lr2 = LogisticRegression(multi_class='multinomial', max_iter=3000, random_state=42)\n",
        "knn2 = KNeighborsClassifier(n_neighbors=20)\n",
        "\n",
        "#Make predictions \n",
        "lr_preds = sentence_embed_train(X_train_p, X_val_p, y_train, y_val, lr2)\n",
        "knn_preds = sentence_embed_train(X_train_p, X_val_p, y_train, y_val, knn2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LHI33_dC-hd",
        "outputId": "05903fd3-40a6-48da-9e96-81bef8f92a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "semeval-tweets/twitter-dev-data.txt (KNN Classifier): 0.552\n"
          ]
        }
      ],
      "source": [
        "#Evaluate knn classifier \n",
        "evaluate(knn_preds, \"semeval-tweets/twitter-dev-data.txt\", \"KNN Classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15gSDH3zFkfL",
        "outputId": "81ffff6c-b23b-401c-a0c0-aef840acf7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "semeval-tweets/twitter-dev-data.txt (Logistic Regression): 0.656\n"
          ]
        }
      ],
      "source": [
        "#Evaluate Logistic Regression\n",
        "evaluate(lr_preds, \"semeval-tweets/twitter-dev-data.txt\", \"Logistic Regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt_8guN3nIrX"
      },
      "source": [
        "### Word embeddings \n",
        "\n",
        "In this section we encode our data with word embedding. We then attempt to improve our benchmarks with logistic regression and Naive Bayes Classifier. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADBkv0bOz-ih"
      },
      "outputs": [],
      "source": [
        "# Load training set, dev set and testing set\n",
        "data = {}\n",
        "tweetids = {}\n",
        "tweetgts = {}\n",
        "tweets = {}\n",
        "\n",
        "for dataset in ['twitter-training-data.txt', 'twitter-dev-data.txt'] + testsets:\n",
        "    dataset_path = join('semeval-tweets', dataset)\n",
        "\n",
        "    data[dataset] = []\n",
        "    tweets[dataset] = []\n",
        "    tweetids[dataset] = []\n",
        "    tweetgts[dataset] = []\n",
        "    # write code to read in the datasets here\n",
        "    \n",
        "    with open(dataset_path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            fields = line.split('\\t')\n",
        "            tweetids[dataset].append(fields[0])\n",
        "            tweetgts[dataset].append(fields[1])\n",
        "            tweets[dataset].append(fields[2].rstrip(\"\\n\"))  #remove line breaks\n",
        "            data[dataset].append(fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJr2bmqiDLlX"
      },
      "outputs": [],
      "source": [
        "#Load GloVe 6B word-embeddings (dimension = 100)\n",
        "with open(\"./glove/glove.6B.100d.txt\",'rt') as file:\n",
        "    glove_content = file.read().strip().split('\\n')\n",
        "\n",
        "#Load the words and their corresponding embeddings into two lists\n",
        "words = [glove_content[i].split(' ')[0] for i in range(len(glove_content))]\n",
        "embeddings = [[float(val) for val in glove_content[i].split(' ')[1:]] for i in range(len(glove_content))]\n",
        "word_dict = {glove_content[i].split(' ')[0]: [float(val) for val in glove_content[i].split(' ')[1:]] for i in range(len(glove_content))}\n",
        "\n",
        "assert len(embeddings) == len(words) == 400001\n",
        "assert len(embeddings[0]) == 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVqwGMiuEA2n"
      },
      "outputs": [],
      "source": [
        "#Define the numpy array versions of the words and the embeddings\n",
        "\n",
        "words_np = np.array(words)\n",
        "embeddings_np = np.array(embeddings)\n",
        "\n",
        "assert embeddings_np.shape == (400001, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl9UW7ItxSVk"
      },
      "outputs": [],
      "source": [
        "#Define embeddings for unknown and padding tokens - zero padding and take mean for unknown tokens\n",
        "unknown_embedding = np.mean(embeddings_np,axis=0,keepdims=True) \n",
        "padding_embedding = np.zeros((1,embeddings_np.shape[1])) \n",
        "\n",
        "#Update the embeddings\n",
        "embeddings_np = np.vstack((padding_embedding, unknown_embedding, embeddings_np))\n",
        "\n",
        "word_dict[\"<PAD>\"] = list(padding_embedding)\n",
        "word_dict[\"<UNK>\"] = list(unknown_embedding)\n",
        "\n",
        "assert embeddings_np.shape == (400003, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiJnnPV_GsrF"
      },
      "outputs": [],
      "source": [
        "#Reload un-processed train and test data\n",
        "X_train = tweets['twitter-training-data.txt']\n",
        "X_val = tweets['twitter-dev-data.txt']\n",
        "\n",
        "#Encode our labels (gts)\n",
        "Encoder = LabelEncoder()\n",
        "\n",
        "y_train = tweetgts['twitter-training-data.txt']\n",
        "y_val = tweetgts['twitter-dev-data.txt']\n",
        "\n",
        "y_train = Encoder.fit_transform(y_train)\n",
        "y_val = Encoder.fit_transform(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwP7BL2GHLb-"
      },
      "outputs": [],
      "source": [
        "#Preprossing before embedding \n",
        "def emb_preprocess(X):\n",
        "\n",
        "    #Instantiate tokenizer for our tweets\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    #Remove url\n",
        "    X_temp = [re.sub('https?:\\/\\/\\S+|www\\.(\\w+\\.)+\\S+', ' ', X[i]) for i in range(len(X))]\n",
        "    #Remove usermention\n",
        "    X_temp = [re.sub('@\\S*', ' ', X_temp[i]) for i in range(len(X_temp))]\n",
        "    #Remove hashtag\n",
        "    X_temp = [re.sub('#\\S*', ' ', X_temp[i]) for i in range(len(X_temp))]\n",
        "    #Remove special characters and punctuations\n",
        "    X_temp = [re.sub('[^A-Za-z0-9 ]+', ' ', X_temp[i]) for i in range(len(X_temp))]\n",
        "    #Tokenize sentences \n",
        "    X_temp = [tokenizer(X_temp[i]) for i in range(len(X_temp))]\n",
        "    #Lowercase (as GloVe contains most words in lowercase e.g. thursday)\n",
        "    X_temp = [[word.lower() for word in X_temp[i]] for i in range(len(X_temp))]\n",
        "\n",
        "    return X_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZMLfJGGHepn"
      },
      "outputs": [],
      "source": [
        "#Preprecess the training and validation data \n",
        "\n",
        "X_train_pp = emb_preprocess(X_train)\n",
        "X_val_pp = emb_preprocess(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ_Lj0mrSU09"
      },
      "outputs": [],
      "source": [
        "#Find the longest sentence to determine max lenth of each document\n",
        "def longest_sentence(X):\n",
        "    max = 0\n",
        "    for i in range(len(X)):\n",
        "        if len(X[i]) > max:\n",
        "            max = len(X[i])\n",
        "    return max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TRnyevuTEf7",
        "outputId": "ed6771ad-ab58-41c8-cb04-179738be01fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "#Longest sentence in training data = 44 - we believe that 50 should be a save choice for max doc leng\n",
        "\n",
        "longest_sentence(X_train_pp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEavE_KJZeoo"
      },
      "outputs": [],
      "source": [
        "#Build a vocab object with the 5000 most frequent tokens from the training set with 2 special tokens\n",
        "vocab = build_vocab_from_iterator(X_train_pp, specials=[\"<PAD>\", \"<UNK>\"], max_tokens=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-jT4957dj7c"
      },
      "outputs": [],
      "source": [
        "#Set default idx to 1, which is <UNK> token\n",
        "vocab.set_default_index(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn4g-e1TdtU2"
      },
      "outputs": [],
      "source": [
        "#Initialize an embedding matrix\n",
        "\n",
        "emb_matrix = np.zeros((5000, 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1lx1-o6fph9"
      },
      "outputs": [],
      "source": [
        "#Special tokents\n",
        "emb_matrix[0] = padding_embedding\n",
        "emb_matrix[1] = unknown_embedding\n",
        "count = 0\n",
        "#Build the embedding matrix for these 5000 tokens\n",
        "for i in range(2, 5000):\n",
        "    if vocab.lookup_token(i) in words:\n",
        "        emb_matrix[i] = np.array(word_dict[vocab.lookup_token(i)])\n",
        "    else:\n",
        "        count += 1\n",
        "        emb_matrix[i] = unknown_embedding\n",
        "assert emb_matrix.shape == (5000, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPUiJ-5R4Ip4"
      },
      "outputs": [],
      "source": [
        "#Convert embedding matrix into torch tensor\n",
        "\n",
        "emb_matrix = torch.from_numpy(emb_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzSroqRIk3q8"
      },
      "outputs": [],
      "source": [
        "#A class of Neural Network consisting of one LSTM layer and one linear layer as required\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_dim=128, num_layers=1, dropout=0, bidirectional=False):    \n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding.from_pretrained(emb_matrix, freeze=True)\n",
        "        self.lstm = nn.LSTM(100, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(hidden_dim, 3)\n",
        "\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        embedded = self.embedding(text).float()     #[batch_size x seq_len x emb_dim]\n",
        "        lstm_output, _ = self.lstm(embedded)        #[batch_size x seq_len x hidden_dim]\n",
        "        output = self.fc(torch.mean(lstm_output, dim=1))       #[batch_size x seq_len x 3]       #Last layer output of dimension [batch_size x 3]\n",
        "                           \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_t7f-acmI9r"
      },
      "outputs": [],
      "source": [
        "#Now we define our own class of Dataset for our text data\n",
        "\n",
        "#Train on GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, target):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.data[idx]\n",
        "        encode = [vocab[word] for word in sentence]\n",
        "        \n",
        "        #Ensure the sentence size = 20\n",
        "        #Pad the sentence if len < 20\n",
        "        while len(encode) < 20:\n",
        "            encode.append(0)\n",
        "        #Truncate the sentence if len > 20\n",
        "        if len(encode) > 20:\n",
        "            encode = encode[:20]\n",
        "        \n",
        "        label = self.target[idx]\n",
        "\n",
        "        #Convert text and label into tensors\n",
        "        text = torch.tensor(encode)\n",
        "        label = torch.tensor(label)\n",
        "\n",
        "        return text, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9JrKiAcm3Ps"
      },
      "outputs": [],
      "source": [
        "#Create datasets for training\n",
        "\n",
        "train_data = TextDataset(X_train_pp, y_train)\n",
        "val_data = TextDataset(X_val_pp, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVJcbROmc3F-"
      },
      "outputs": [],
      "source": [
        "#Create Dataloaders \n",
        "#Note that our dataloader iteration has first dimension = batch size\n",
        "#So for the LSTM we have to set batch_first = True\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm792QaroC_k"
      },
      "outputs": [],
      "source": [
        "#Train on GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#CE Loss for multi-class classification\n",
        "criterion_lstm = nn.CrossEntropyLoss()\n",
        "\n",
        "#Instantiate our first model\n",
        "model_lstm = LSTM()\n",
        "model_lstm = model_lstm.to(device)\n",
        "\n",
        "#Adam optimizer with a standardize lr of 0.003\n",
        "optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwdngwmnXKg9"
      },
      "outputs": [],
      "source": [
        "#Define dictionaries to ease access of dataloaders and dataset sizes\n",
        "\n",
        "dataloaders = {\"train\": train_dataloader,\n",
        "               \"val\": val_dataloader}\n",
        "\n",
        "dataset_sizes = {\"train\": len(X_train_pp),\n",
        "                 \"val\": len(X_val_pp)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba5hp9Px45W4"
      },
      "outputs": [],
      "source": [
        "#Train the model \n",
        "#Source: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "\n",
        "def train_model(model, criterion, optimizer, num_epochs=10):\n",
        "\n",
        "    #Track best accuracy\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    #Track the losses for convergence plots\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        #Train and validation phase for each epoch\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  #Training mode\n",
        "            else:\n",
        "                model.eval()   #Evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            #Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                #Zero out the gradients before the forward pass\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "\n",
        "                #Forward pass only gradient enabled only when training\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    #Backprop and optimizing step\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                #Track loss\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "            elif phase == 'val':\n",
        "                val_loss.append(epoch_loss)\n",
        "            \n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "            \n",
        "            # Copy the best model \n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    #Plot the convergence plots of the losses\n",
        "    plt.plot(np.array(train_loss), color='green', label='Training Loss')\n",
        "    plt.plot(np.array(val_loss), color='blue', label='Validation Loss')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title(\"Convergence Plots of Training and Validation Losses\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Cross Entropy Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    #Save the best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    torch.save(model, \"LSTM\")\n",
        "    \n",
        "    #Evaluate on validation set\n",
        "    model.eval() \n",
        "    for inputs, labels in dataloaders['val']:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        #Zero out the gradients before the forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        preds = Encoder.inverse_transform(preds.cpu().numpy())\n",
        "\n",
        "        #Zip the two lists of ids and our predictions into a single dictionary for evaludation\n",
        "        id_preds = {tweetids['twitter-dev-data.txt'][i]: preds[i] for i in range(len(preds))}\n",
        "    \n",
        "         # #Evaluate on the macro-F1\n",
        "        evaluate(id_preds, \"semeval-tweets/twitter-dev-data.txt\", \"LSTM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lS6N-GprqMAL",
        "outputId": "8c7c3e85-9153-455d-92ee-79149067a242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 0.8676 Acc: 0.5817\n",
            "val Loss: 0.7915 Acc: 0.6305\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 0.7986 Acc: 0.6253\n",
            "val Loss: 0.7810 Acc: 0.6350\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.7646 Acc: 0.6468\n",
            "val Loss: 0.7639 Acc: 0.6385\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.7375 Acc: 0.6624\n",
            "val Loss: 0.7496 Acc: 0.6575\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.7077 Acc: 0.6792\n",
            "val Loss: 0.7571 Acc: 0.6470\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.6719 Acc: 0.7002\n",
            "val Loss: 0.7675 Acc: 0.6450\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.6341 Acc: 0.7194\n",
            "val Loss: 0.7872 Acc: 0.6360\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.5869 Acc: 0.7445\n",
            "val Loss: 0.8039 Acc: 0.6350\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.5407 Acc: 0.7681\n",
            "val Loss: 0.8694 Acc: 0.6305\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.4885 Acc: 0.7925\n",
            "val Loss: 0.9277 Acc: 0.6265\n",
            "Best val Acc: 0.657500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "semeval-tweets/twitter-dev-data.txt (LSTM): 0.627\n"
          ]
        }
      ],
      "source": [
        "train_model(model_lstm, criterion_lstm, optimizer_lstm, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyJFTI1c9_fZ"
      },
      "source": [
        "## BERT finetuning\n",
        "\n",
        "Finally we finetune a pre-trained BERT and perform fintuning on our downstream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1v0hbX0-L5v"
      },
      "outputs": [],
      "source": [
        "#Preprocessing for Distil-BERT\n",
        "def bert_preprocess(X):\n",
        "\n",
        "    #Remove url\n",
        "    X_temp = [re.sub('https?:\\/\\/\\S+|www\\.(\\w+\\.)+\\S+', '', X[i]) for i in range(len(X))]\n",
        "    #Remove usermention\n",
        "    X_temp = [re.sub('@\\S*', '', X_temp[i]) for i in range(len(X_temp))]\n",
        "    #Remove hashtag\n",
        "    X_temp = [re.sub('#\\S*', '', X_temp[i]) for i in range(len(X_temp))]\n",
        "\n",
        "    return X_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpA-FnKacALA"
      },
      "outputs": [],
      "source": [
        "#Just do the simplest preprecessing \n",
        "X_train_bert = bert_preprocess(X_train)\n",
        "X_val_bert = bert_preprocess(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEhSSVR49LK_"
      },
      "outputs": [],
      "source": [
        "#BERT Tokenizer\n",
        "bert_tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3X_DfD09SyF"
      },
      "outputs": [],
      "source": [
        "#BERT dataset\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, data, target, tokenizer, max_length=100):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        text = self.data[idx]\n",
        "        \n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            padding='max_length',\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'target': torch.tensor(self.target[idx], dtype=torch.long)\n",
        "            }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9NbApRen37z"
      },
      "outputs": [],
      "source": [
        "#Create BERT datasets \n",
        "train_dataset_bert = BERTDataset(X_train_bert, y_train, bert_tokenizer, max_length=100)\n",
        "val_dataset_bert = BERTDataset(X_val_bert, y_val, bert_tokenizer, max_length=100)\n",
        "\n",
        "#Create Dataloaders \n",
        "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=32, shuffle=True)\n",
        "val_dataloader_bert = DataLoader(val_dataset_bert, batch_size=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nImh6NGUz2Vp"
      },
      "outputs": [],
      "source": [
        "#Define the model class for BERT\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.bert_model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.fc = nn.Linear(768, 3)\n",
        "        \n",
        "    def forward(self,ids,mask,token_type_ids):\n",
        "        _, bert_output= self.bert_model(ids,attention_mask=mask,token_type_ids=token_type_ids, return_dict=False)\n",
        "        output = self.fc(bert_output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAaPbWDI7MfA",
        "outputId": "3313b0f4-13c9-470d-bff7-9a72207431b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#Train on GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Instantiate model\n",
        "model_bert = BERT().to(device)\n",
        "\n",
        "#Cross Entropy Loss\n",
        "criterion_bert = nn.CrossEntropyLoss()\n",
        "\n",
        "#Initialize Optimizer\n",
        "optimizer_bert = torch.optim.Adam(model_bert.parameters(), lr=0.0002)\n",
        "\n",
        "#Freeze half of the BERT layers\n",
        "ct = 0\n",
        "for child in model_bert.bert_model.children():\n",
        "    ct += 1\n",
        "    if ct < 8:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqtJV_Vdt1Es"
      },
      "outputs": [],
      "source": [
        "#Define dictionaries to ease access of dataloaders and dataset sizes\n",
        "\n",
        "dataloaders = {\"train\": train_dataloader_bert,\n",
        "               \"val\": val_dataloader_bert}\n",
        "\n",
        "dataset_sizes = {\"train\": len(X_train_bert),\n",
        "                 \"val\": len(X_val_bert)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePDGHNQB7WdX"
      },
      "outputs": [],
      "source": [
        "#Finetune the DistilBERT model \n",
        "\n",
        "def finetune_bert(model, criterion, optimizer, num_epochs=10):\n",
        "\n",
        "    #Track best accuracy\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    #Track the losses for convergence plots\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        #Train and validation phase for each epoch\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  #Training mode\n",
        "            else:\n",
        "                model.eval()   #Evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            #Iterate over data\n",
        "            for dl in dataloaders[phase]:\n",
        "\n",
        "                #Retrieve data information from batch\n",
        "                ids=dl['ids'].to(device)\n",
        "                token_type_ids=dl['token_type_ids'].to(device)\n",
        "                mask= dl['mask'].to(device)\n",
        "                label=dl['target'].to(device)\n",
        "\n",
        "                #Zero out the gradients before the forward pass\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "\n",
        "                #Forward pass only gradient enabled only when training\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, label)\n",
        "\n",
        "                    #Backprop and optimizing step\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                #Track loss\n",
        "                running_loss += loss.item() * ids.size(0)\n",
        "                running_corrects += torch.sum(preds == label.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "            elif phase == 'val':\n",
        "                val_loss.append(epoch_loss)\n",
        "            \n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "            \n",
        "            # Copy the best model \n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    #Plot the convergence plots of the losses\n",
        "    plt.plot(np.array(train_loss), color='green', label='Training Loss')\n",
        "    plt.plot(np.array(val_loss), color='blue', label='Validation Loss')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title(\"Convergence Plots of Training and Validation Losses\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Cross Entropy Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    #Save the best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    torch.save(model, \"BERT\")\n",
        "    \n",
        "    #Evaluate on validation set\n",
        "    model.eval() \n",
        "    for dl in dataloaders['val']:\n",
        "\n",
        "        #Retrieve data information from batch\n",
        "        ids=dl['ids'].to(device)\n",
        "        token_type_ids=dl['token_type_ids'].to(device)\n",
        "        mask= dl['mask'].to(device)\n",
        "        label=dl['target'].to(device)\n",
        "\n",
        "        #Zero out the gradients before the forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        preds = Encoder.inverse_transform(preds.cpu().numpy())\n",
        "\n",
        "        #Zip the two lists of ids and our predictions into a single dictionary for evaludation\n",
        "        id_preds = {tweetids['twitter-dev-data.txt'][i]: preds[i] for i in range(len(preds))}\n",
        "    \n",
        "        #Evaluate on the macro-F1\n",
        "        evaluate(id_preds, \"semeval-tweets/twitter-dev-data.txt\", \"BERT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "czyPEKqXwM9M",
        "outputId": "9f427457-e560-4309-acd5-4bae06eb77b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss: 0.9729 Acc: 0.5198\n",
            "val Loss: 0.9246 Acc: 0.5545\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss: 0.9124 Acc: 0.5610\n",
            "val Loss: 0.8893 Acc: 0.5675\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss: 0.8905 Acc: 0.5722\n",
            "val Loss: 0.8756 Acc: 0.5665\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss: 0.8753 Acc: 0.5798\n",
            "val Loss: 0.8542 Acc: 0.5935\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss: 0.8656 Acc: 0.5866\n",
            "val Loss: 0.8525 Acc: 0.5855\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss: 0.8559 Acc: 0.5928\n",
            "val Loss: 0.8389 Acc: 0.5955\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss: 0.8504 Acc: 0.5946\n",
            "val Loss: 0.8287 Acc: 0.6035\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss: 0.8440 Acc: 0.6004\n",
            "val Loss: 0.8274 Acc: 0.6015\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss: 0.8426 Acc: 0.6003\n",
            "val Loss: 0.8243 Acc: 0.6015\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss: 0.8382 Acc: 0.6036\n",
            "val Loss: 0.8210 Acc: 0.6045\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss: 0.8352 Acc: 0.6020\n",
            "val Loss: 0.8140 Acc: 0.6110\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss: 0.8328 Acc: 0.6056\n",
            "val Loss: 0.8132 Acc: 0.6095\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss: 0.8300 Acc: 0.6074\n",
            "val Loss: 0.8077 Acc: 0.6175\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss: 0.8294 Acc: 0.6066\n",
            "val Loss: 0.8059 Acc: 0.6115\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss: 0.8278 Acc: 0.6081\n",
            "val Loss: 0.8015 Acc: 0.6190\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss: 0.8243 Acc: 0.6068\n",
            "val Loss: 0.7996 Acc: 0.6220\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss: 0.8216 Acc: 0.6099\n",
            "val Loss: 0.8020 Acc: 0.6195\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss: 0.8200 Acc: 0.6118\n",
            "val Loss: 0.7998 Acc: 0.6235\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss: 0.8185 Acc: 0.6145\n",
            "val Loss: 0.7950 Acc: 0.6320\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss: 0.8167 Acc: 0.6120\n",
            "val Loss: 0.7919 Acc: 0.6265\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss: 0.8172 Acc: 0.6140\n",
            "val Loss: 0.8020 Acc: 0.6175\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss: 0.8164 Acc: 0.6142\n",
            "val Loss: 0.7917 Acc: 0.6265\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss: 0.8138 Acc: 0.6137\n",
            "val Loss: 0.7963 Acc: 0.6260\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss: 0.8156 Acc: 0.6108\n",
            "val Loss: 0.7875 Acc: 0.6270\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss: 0.8150 Acc: 0.6151\n",
            "val Loss: 0.7927 Acc: 0.6195\n",
            "Best val Acc: 0.632000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "semeval-tweets/twitter-dev-data.txt (BERT): 0.605\n"
          ]
        }
      ],
      "source": [
        "finetune_bert(model_bert, criterion_bert, optimizer_bert, num_epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUL7o1nOgLt6"
      },
      "source": [
        "#### Build sentiment classifiers\n",
        "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB8OsQR3gLt7",
        "outputId": "7ff1828d-e9b2-4b0c-c0f4-cb81f85420c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression\n",
            "semeval-tweets/twitter-test1.txt (BoW-Logistic Regression): 0.467\n",
            "semeval-tweets/twitter-test2.txt (BoW-Logistic Regression): 0.446\n",
            "semeval-tweets/twitter-test3.txt (BoW-Logistic Regression): 0.464\n",
            "Training Logistic Regression\n",
            "semeval-tweets/twitter-test1.txt (Sentence Embedding-Logistic Regression): 0.569\n",
            "semeval-tweets/twitter-test2.txt (Sentence Embedding-Logistic Regression): 0.594\n",
            "semeval-tweets/twitter-test3.txt (Sentence Embedding-Logistic Regression): 0.565\n",
            "Training Naive Bayes Classifier\n",
            "semeval-tweets/twitter-test1.txt (BoW-Naive Bayes Classifier): 0.413\n",
            "semeval-tweets/twitter-test2.txt (BoW-Naive Bayes Classifier): 0.421\n",
            "semeval-tweets/twitter-test3.txt (BoW-Naive Bayes Classifier): 0.444\n",
            "Training KNN Classifier\n",
            "semeval-tweets/twitter-test1.txt (BoW-KNN Classifier): 0.237\n",
            "semeval-tweets/twitter-test2.txt (BoW-KNN Classifier): 0.268\n",
            "semeval-tweets/twitter-test3.txt (BoW-KNN Classifier): 0.238\n",
            "Training KNN Classifier\n",
            "semeval-tweets/twitter-test1.txt (Sentence Embedding-KNN Classifier): 0.428\n",
            "semeval-tweets/twitter-test2.txt (Sentence Embedding-KNN Classifier): 0.454\n",
            "semeval-tweets/twitter-test3.txt (Sentence Embedding-KNN Classifier): 0.416\n",
            "Training LSTM\n",
            "semeval-tweets/twitter-test1.txt (GloVe-LSTM): 0.589\n",
            "semeval-tweets/twitter-test2.txt (GloVe-LSTM): 0.588\n",
            "semeval-tweets/twitter-test3.txt (GloVe-LSTM): 0.556\n",
            "Training BERT\n",
            "semeval-tweets/twitter-test1.txt (BERT-BERT): 0.518\n",
            "semeval-tweets/twitter-test2.txt (BERT-BERT): 0.573\n",
            "semeval-tweets/twitter-test3.txt (BERT-BERT): 0.530\n"
          ]
        }
      ],
      "source": [
        "#Evaluate our models on test datasets\n",
        "\n",
        "for classifier in ['Logistic Regression', 'Naive Bayes Classifier', 'KNN Classifier', 'LSTM', 'BERT']:\n",
        "    for features in ['BoW', 'Sentence Embedding', \"GloVe\", 'BERT']:\n",
        "        \n",
        "        if classifier == 'Logistic Regression':\n",
        "            if features == \"BoW\":\n",
        "                model = LogisticRegression(multi_class='multinomial', max_iter=3000, random_state=42)\n",
        "                tfidf_vector = TfidfVectorizer()\n",
        "                X_train_tf = tfidf_vector.fit_transform(X_train_bow)\n",
        "                model.fit(X_train_tf, y_train)\n",
        "\n",
        "                print('Training ' + classifier)\n",
        "                \n",
        "            elif features == \"Sentence Embedding\":\n",
        "                model = LogisticRegression(multi_class='multinomial', max_iter=3000, random_state=42)\n",
        "                pretrained = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "                X_train_embeddings = pretrained.encode(X_train_p)\n",
        "                model.fit(X_train_embeddings, y_train)\n",
        "\n",
        "                print('Training ' + classifier)\n",
        "            else:\n",
        "                continue\n",
        "        \n",
        "        elif classifier == 'Naive Bayes Classifier':\n",
        "            if features == \"BoW\":\n",
        "                model = MultinomialNB(fit_prior=False)\n",
        "                tfidf_vector = TfidfVectorizer()\n",
        "                X_train_tf = tfidf_vector.fit_transform(X_train_bow)\n",
        "                model.fit(X_train_tf, y_train)\n",
        "                print('Training ' + classifier)\n",
        "            else:\n",
        "                continue\n",
        "        \n",
        "            \n",
        "        elif classifier == 'KNN Classifier':\n",
        "            if features == \"BoW\":\n",
        "                model = KNeighborsClassifier(n_neighbors=20)\n",
        "                tfidf_vector = TfidfVectorizer()\n",
        "                X_train_tf = tfidf_vector.fit_transform(X_train_bow)\n",
        "                model.fit(X_train_tf, y_train)\n",
        "                print('Training ' + classifier)\n",
        "\n",
        "            elif features == \"Sentence Embedding\":\n",
        "                model = KNeighborsClassifier(n_neighbors=20)\n",
        "                pretrained = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "                X_train_embeddings = pretrained.encode(X_train_p)\n",
        "                model.fit(X_train_embeddings, y_train)\n",
        "                print('Training ' + classifier)\n",
        "            else:\n",
        "                continue\n",
        "        \n",
        "        elif classifier == 'LSTM':\n",
        "            if features == 'GloVe':\n",
        "                #Run this line of code if model is not downloaded\n",
        "                # train_model(model_lstm, criterion_lstm, optimizer_lstm, num_epochs=10)\n",
        "                model = torch.load(\"LSTM\")\n",
        "                print('Training ' + classifier)\n",
        "            else:\n",
        "                continue\n",
        "        \n",
        "        elif classifier == 'BERT':\n",
        "            if features == \"BERT\":\n",
        "                #Run this line of code if model is not downloaded\n",
        "                # finetune_bert(model_bert, criterion_bert, optimizer_bert, num_epochs=15)\n",
        "                model = torch.load(\"BERT\")\n",
        "                print('Training ' + classifier)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        else:\n",
        "            print('Unknown classifier name' + classifier)\n",
        "            continue\n",
        "\n",
        "        # Predition performance of the classifiers\n",
        "        for testset in testsets:\n",
        "            if features == \"Sentence Embedding\":\n",
        "                X_test_embedding = pretrained.encode(sentence_preprocess(tweets[testset]))\n",
        "                pred = model.predict(X_test_embedding)\n",
        "                pred = Encoder.inverse_transform(pred)\n",
        "                \n",
        "                #Make predictions on our developing data and transform it back to the original label\n",
        "                id_preds = {tweetids[testset][i]: pred[i] for i in range(len(pred))}\n",
        "\n",
        "                testset_name = testset\n",
        "                testset_path = join('semeval-tweets', testset_name)\n",
        "                evaluate(id_preds, testset_path, features + '-' + classifier)\n",
        "            \n",
        "            elif features == \"BoW\":\n",
        "                X_test_bow = tfidf_vector.transform(sentence_preprocess(tweets[testset]))\n",
        "                pred = model.predict(X_test_bow)\n",
        "                pred = Encoder.inverse_transform(pred)\n",
        "                \n",
        "                #Make predictions on our developing data and transform it back to the original label\n",
        "                id_preds = {tweetids[testset][i]: pred[i] for i in range(len(pred))}\n",
        "\n",
        "                testset_name = testset\n",
        "                testset_path = join('semeval-tweets', testset_name)\n",
        "                evaluate(id_preds, testset_path, features + '-' + classifier)\n",
        "            \n",
        "            elif features == \"GloVe\":\n",
        "                X_test_pp = emb_preprocess(tweets[testset])\n",
        "                test_data = TextDataset(X_test_pp, Encoder.transform(tweetgts[testset]))\n",
        "                test_dataloader = DataLoader(test_data, batch_size=len(tweets[testset]))\n",
        "                \n",
        "                model.eval() \n",
        "\n",
        "                for inputs, labels in test_dataloader:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    #Zero out the gradients before the forward pass\n",
        "                    optimizer_lstm.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    preds = Encoder.inverse_transform(preds.cpu().numpy())\n",
        "\n",
        "                    #Zip the two lists of ids and our predictions into a single dictionary for evaludation\n",
        "                    id_preds = {tweetids[testset][i]: preds[i] for i in range(len(preds))}\n",
        "\n",
        "                    testset_name = testset\n",
        "                    testset_path = join('semeval-tweets', testset_name)\n",
        "                    evaluate(id_preds, testset_path, features + '-' + classifier)\n",
        "\n",
        "            elif features == \"BERT\":\n",
        "                X_test_bert = bert_preprocess(tweets[testset])\n",
        "                test_data = BERTDataset(X_test_bert, Encoder.transform(tweetgts[testset]), bert_tokenizer, max_length=100) \n",
        "                test_dataloader = DataLoader(test_data, batch_size=len(tweets[testset])) \n",
        "                    \n",
        "                model.eval() \n",
        "\n",
        "                for dl in test_dataloader:\n",
        "\n",
        "                    #Retrieve data information from batch\n",
        "                    ids=dl['ids'].to(device)\n",
        "                    token_type_ids=dl['token_type_ids'].to(device)\n",
        "                    mask= dl['mask'].to(device)\n",
        "                    label=dl['target'].to(device)\n",
        "\n",
        "                    #Zero out the gradients before the forward pass\n",
        "                    optimizer_bert.zero_grad()\n",
        "                    outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    preds = Encoder.inverse_transform(preds.cpu().numpy())\n",
        "\n",
        "                    #Zip the two lists of ids and our predictions into a single dictionary for evaludation\n",
        "                    id_preds = {tweetids[testset][i]: preds[i] for i in range(len(preds))}\n",
        "\n",
        "                    testset_name = testset\n",
        "                    testset_path = join('semeval-tweets', testset_name)\n",
        "                    evaluate(id_preds, testset_path, features + '-' + classifier)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}